{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from gl_clip import clip\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.feature_extract import Feature_Extractor\n",
    "from modules.data_loader import WeatherDataset\n",
    "from modules.classifier import MWSC\n",
    "from modules.metric import Metric\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ablation_mode = 1\n",
    "batch_size = 64\n",
    "clip_base_model = 'ViT-B/32'\n",
    "\n",
    "image_path = '/data/MWSC/data/'\n",
    "test_label_path = '/data/MWSC/data/label/test_data.csv'\n",
    "state_dict_path = '/data/MWSC/result/mwsc_ablation_mode_1_ViT_B_32.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = Feature_Extractor(device, clip_base_model)\n",
    "test_dataset = WeatherDataset(test_label_path, transform=feature.preprocess, data_dir=image_path)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "weather_types = test_dataset.weather_types\n",
    "severity_levels = test_dataset.severity_levels\n",
    "\n",
    "prompts = test_dataset.prompts\n",
    "text_inputs = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "\n",
    "metric = Metric(weather_types, severity_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MWSC(clip_base_model, len(weather_types), len(severity_levels), ablation_mode)\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "## If you want to measure fps, uncomment the comments below and change the batch size to 1.\n",
    "starter = torch.cuda.Event(enable_timing=True)\n",
    "ender = torch.cuda.Event(enable_timing=True)\n",
    "total_time = 0\n",
    "idx= 0\n",
    "\n",
    "all_weather_probs = []\n",
    "all_weather_labels = []\n",
    "all_severity_probs = []\n",
    "all_severity_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, weather_labels, severity_labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        weather_labels = weather_labels.to(device)\n",
    "        severity_labels = severity_labels.to(device)\n",
    "        \n",
    "        starter.record()\n",
    "\n",
    "        global_feat, local_feat, text_features = feature(images, text_inputs)\n",
    "\n",
    "        weather_out, severity_out = model(global_feat, local_feat, text_features)\n",
    "        \n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        weather_probs = torch.sigmoid(weather_out).cpu().numpy()\n",
    "        all_weather_probs.extend(weather_probs)\n",
    "        all_weather_labels.extend(weather_labels.cpu().numpy())\n",
    "\n",
    "        severity_probs = F.softmax(severity_out, dim=1).cpu().numpy()\n",
    "        all_severity_probs.extend(severity_probs)\n",
    "        all_severity_labels.extend(severity_labels.cpu().numpy())\n",
    "        \n",
    "        infer_time = starter.elapsed_time(ender)\n",
    "        total_time+=infer_time\n",
    "        idx+=1\n",
    "\n",
    "weather_metrics = metric.calculate(np.array(all_weather_probs), np.array(all_weather_labels), is_multilabel=True)\n",
    "severity_metrics = metric.calculate(np.array(all_severity_probs), np.array(all_severity_labels), is_multilabel=False)\n",
    "\n",
    "print(f\"Trainable Parameters : {trainable_parameters}\")\n",
    "print(f'Weather Accuracy: {weather_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'Weather mAP:{weather_metrics[\"map\"]:.4f}')\n",
    "print(f'Severity mAP:{severity_metrics[\"map\"]:.4f}')\n",
    "print('Weather Classification Report:')\n",
    "print(weather_metrics['classification_report'])\n",
    "print('Severity Classification Report:')\n",
    "print(severity_metrics['classification_report'])\n",
    "print('Weather Confusion Matrix:')\n",
    "print(weather_metrics['confusion_matrix'])\n",
    "print('\\nSeverity Confusion Matrix:')\n",
    "print(severity_metrics['confusion_matrix'])\n",
    "print('-' * 50)\n",
    "\n",
    "avg_time_per_iter  = total_time/ idx\n",
    "avg_time_per_iter_sec = avg_time_per_iter * 1e-3\n",
    "avg_fps = 1 / avg_time_per_iter_sec\n",
    "\n",
    "print(f\"Avg time per iteration (ms): {avg_time_per_iter:.2f} ms\")\n",
    "print(f\"Avg time per iteration (sec): {avg_time_per_iter_sec:.4f} s\")\n",
    "print(f\"FPS: {avg_fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "for i in range(4):\n",
    "    \n",
    "    cm = weather_metrics['confusion_matrix'][i]\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    disp.ax_.set_title(f\"Weather Confusion Matrix - {weather_types[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm = severity_metrics['confusion_matrix']\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=severity_levels)\n",
    "disp.plot()\n",
    "disp.ax_.set_title(\"Severity Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.classification import multiclass_calibration_error\n",
    "from torch import tensor\n",
    "all_severity_probs = tensor(all_severity_probs)\n",
    "all_severity_labels = tensor(all_severity_labels)\n",
    "print(multiclass_calibration_error(all_severity_probs, all_severity_labels, num_classes=3, n_bins=3, norm='l1'))\n",
    "# print(multiclass_calibration_error(all_severity_probs, all_severity_labels, num_classes=3, n_bins=3, norm='l2'))\n",
    "# print(multiclass_calibration_error(all_severity_probs, all_severity_labels, num_classes=3, n_bins=3, norm='max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import randn, randint\n",
    "# Example plotting a multiple values\n",
    "from torchmetrics.classification import MulticlassCalibrationError\n",
    "metric = MulticlassCalibrationError(num_classes=3, n_bins=3, norm='l1')\n",
    "\n",
    "values = []\n",
    "for _ in range(len(all_severity_labels)):\n",
    "    values.append(metric(all_severity_probs, all_severity_labels))\n",
    "fig_, ax_ = metric.plot(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
